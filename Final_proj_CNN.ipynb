{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Sarcasm Recognition\n",
    "  \n",
    "  #### Class: CS660\n",
    "  #### Student: \n",
    "  * Shuhui Wu\n",
    "  * Shuqi Gao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten, Dropout, Reshape\n",
    "from keras.layers import Input, Embedding, Concatenate\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras import regularizers\n",
    "from keras.initializers import Constant\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Prepocessing\n",
    "* Tokenize and embed docs using pretrained Glove embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_dict(path):\n",
    "    embedding_dict = dict()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            glove_vecs = line.split()\n",
    "            key = glove_vecs[0]\n",
    "            coefs = np.asarray(glove_vecs[1:], dtype='float32')\n",
    "            embedding_dict[key] = coefs\n",
    "    return embedding_dict\n",
    "\n",
    "\n",
    "def preprocessing(texts, labels, tknzr, maxlen):\n",
    "    \n",
    "    enc_text = tknzr.texts_to_sequences(texts)\n",
    "    Xs = pad_sequences(enc_text, maxlen=maxlen, padding='post')\n",
    "        \n",
    "    return Xs, labels\n",
    "\n",
    "# preprocessing('train-balanced-sarcasm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "* training and testing data is too big to fit in the RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(cols, batch_size, embedding_dict, tknzr, maxlen):\n",
    "    while True:\n",
    "        ind = 0\n",
    "        cols = cols.sample(frac=1).reset_index(drop=True)\n",
    "        texts = cols['comment'].astype(str)\n",
    "        labels = cols['label'].astype(int)\n",
    "        \n",
    "        while ind < texts.size:\n",
    "            if ind + batch_size < texts.size:\n",
    "                batch_texts = texts[ind: ind+batch_size+1]\n",
    "                batch_labels = labels[ind: ind+batch_size+1]\n",
    "            else:\n",
    "                batch_texts = texts[ind: texts.size+1]\n",
    "                batch_labels = labels[ind: texts.size+1]\n",
    "\n",
    "            Xs, ys = preprocessing(batch_texts, batch_labels, tknzr, maxlen)\n",
    "            ind += batch_size\n",
    "            yield Xs, ys\n",
    "# print(data_generator('train-balanced-sarcasm.csv', 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(weights, maxlen, vocab_size):\n",
    "    input_layer = Input(shape=(maxlen,), dtype='int32')\n",
    "    e_layer = Embedding(vocab_size, 100, embeddings_initializer=Constant(weights), input_length=maxlen, trainable=True)(input_layer)\n",
    "    reshape = Reshape(((maxlen, 100, 1)))(e_layer)\n",
    "    conv_1 = Conv2D(100, kernel_size=(5, 100), activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "    conv_2 = Conv2D(100, kernel_size=(6, 100), activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "    conv_3 = Conv2D(100, kernel_size=(7, 100), activation='relu', kernel_initializer='normal', kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "\n",
    "    maxpool_1 = MaxPool2D(pool_size=(maxlen-5+1, 1), padding='valid')(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(maxlen-6+1, 1), padding='valid')(conv_2)\n",
    "    maxpool_3 = MaxPool2D(pool_size=(maxlen-7+1, 1), padding='valid')(conv_3)\n",
    "\n",
    "    conca_tensor = Concatenate(axis=1)([maxpool_1, maxpool_2, maxpool_3])\n",
    "    flatten = Flatten()(conca_tensor)\n",
    "    dropout_2 = Dropout(0.5)(flatten)\n",
    "    output = Dense(1, activation='sigmoid')(dropout_2)\n",
    "\n",
    "    model = Model(input_layer, output)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10000, 100)   18058400    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 10000, 100, 1 0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 9996, 1, 100) 50100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 9995, 1, 100) 60100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 9994, 1, 100) 70100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 1, 100)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 300)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 300)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            301         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 18,239,001\n",
      "Trainable params: 18,239,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 3603s 225ms/step - loss: 3.3424 - acc: 0.5524 - val_loss: 0.6741 - val_acc: 0.6537\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 3583s 224ms/step - loss: 0.6840 - acc: 0.6284 - val_loss: 0.6845 - val_acc: 0.6278\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 3583s 224ms/step - loss: 0.6824 - acc: 0.6402 - val_loss: 0.6636 - val_acc: 0.6624\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 3583s 224ms/step - loss: 0.6807 - acc: 0.6467 - val_loss: 0.6744 - val_acc: 0.6539\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 3583s 224ms/step - loss: 0.6791 - acc: 0.6500 - val_loss: 0.6570 - val_acc: 0.6667\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 3583s 224ms/step - loss: 0.6781 - acc: 0.6540 - val_loss: 0.6640 - val_acc: 0.6744\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 3583s 224ms/step - loss: 0.6768 - acc: 0.6561 - val_loss: 0.6531 - val_acc: 0.6765\n",
      "Epoch 8/10\n",
      "15999/16000 [============================>.] - ETA: 0s - loss: 0.6764 - acc: 0.6573"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    dataset = pd.read_csv('train-balanced-sarcasm.csv')\n",
    "    cols = dataset[['comment', 'label']]\n",
    "    \n",
    "    batch_size = 50\n",
    "    maxlen = len(max(cols['comment'].astype(str), key=len))\n",
    "    tknzr = Tokenizer()\n",
    "    tknzr.fit_on_texts(cols['comment'].astype(str))\n",
    "    vocab_size = len(tknzr.word_index) + 1\n",
    "    \n",
    "    embedding_dict = glove_dict('glove.6B.100d.txt')\n",
    "    weights = np.zeros((vocab_size, 100))\n",
    "    for word, i in tknzr.word_index.items():\n",
    "        vec = embedding_dict.get(word)\n",
    "        if vec is not None:\n",
    "            weights[i] = vec\n",
    "        else:\n",
    "            weights[i] = np.random.randn(100)\n",
    "    \n",
    "    model = build_model(weights, maxlen, vocab_size)\n",
    "    history = model.fit_generator(data_generator(cols[:800000], batch_size, embedding_dict, tknzr, maxlen), \n",
    "                        steps_per_epoch= 800000 // batch_size,\n",
    "                        epochs=10,\n",
    "                        validation_data=data_generator(cols[800000:900000], batch_size, embedding_dict, tknzr, maxlen),\n",
    "                        validation_steps=100000 // batch_size\n",
    "                       )\n",
    "    acc = model.evaluate_generator(data_generator(cols[900000:], batch_size, embedding_dict, tknzr, maxlen),\n",
    "                                   steps=len(cols[900000:]) // batch_size)\n",
    "    print(acc)\n",
    "    return model, history\n",
    "        \n",
    "        \n",
    "model, history = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d5cfdad43e1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "model.save('CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 3562s 223ms/step - loss: 0.6765 - acc: 0.6641 - val_loss: 0.6614 - val_acc: 0.6800\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 3547s 222ms/step - loss: 0.6746 - acc: 0.6663 - val_loss: 0.6674 - val_acc: 0.6736\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 3544s 221ms/step - loss: 0.6740 - acc: 0.6678 - val_loss: 0.6713 - val_acc: 0.6585\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 3543s 221ms/step - loss: 0.6734 - acc: 0.6682 - val_loss: 0.6808 - val_acc: 0.6483\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 3542s 221ms/step - loss: 0.6727 - acc: 0.6697 - val_loss: 0.6702 - val_acc: 0.6685\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 3543s 221ms/step - loss: 0.6721 - acc: 0.6711 - val_loss: 0.6642 - val_acc: 0.6797\n",
      "Epoch 7/10\n",
      "  643/16000 [>.............................] - ETA: 53:47 - loss: 0.6690 - acc: 0.6744"
     ]
    }
   ],
   "source": [
    "model = load_model('CNN.h5')\n",
    "dataset = pd.read_csv('train-balanced-sarcasm.csv')\n",
    "cols = dataset[['comment', 'label']]\n",
    "\n",
    "batch_size = 50\n",
    "maxlen = len(max(cols['comment'].astype(str), key=len))\n",
    "tknzr = Tokenizer()\n",
    "tknzr.fit_on_texts(cols['comment'].astype(str))\n",
    "vocab_size = len(tknzr.word_index) + 1\n",
    "\n",
    "embedding_dict = glove_dict('glove.6B.100d.txt')\n",
    "weights = np.zeros((vocab_size, 100))\n",
    "for word, i in tknzr.word_index.items():\n",
    "    vec = embedding_dict.get(word)\n",
    "    if vec is not None:\n",
    "        weights[i] = vec\n",
    "    else:\n",
    "        weights[i] = np.random.randn(100)\n",
    "\n",
    "history = model.fit_generator(data_generator(cols[:800000], batch_size, embedding_dict, tknzr, maxlen), \n",
    "                        steps_per_epoch= 800000 // batch_size,\n",
    "                        epochs=10,\n",
    "                        validation_data=data_generator(cols[800000:900000], batch_size, embedding_dict, tknzr, maxlen),\n",
    "                        validation_steps=100000 // batch_size\n",
    "                       )\n",
    "acc = model.evaluate_generator(data_generator(cols[900000:], batch_size, embedding_dict, tknzr, maxlen),\n",
    "                                steps=len(cols[900000:]) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DeepNLP]",
   "language": "python",
   "name": "conda-env-DeepNLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
